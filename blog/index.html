
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal Webpage of Gabriel Iturra-Bocaz">
      
      
      
        <link rel="canonical" href="https://giturra.cl/blog/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.42">
    
    
      
        <title>Blog - Gabriel Iturra-Bocaz</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/stylesheet.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#blog" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Gabriel Iturra-Bocaz" class="md-header__button md-logo" aria-label="Gabriel Iturra-Bocaz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Gabriel Iturra-Bocaz
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Blog
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Gabriel Iturra-Bocaz" class="md-nav__button md-logo" aria-label="Gabriel Iturra-Bocaz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Gabriel Iturra-Bocaz
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../news/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    News
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../bio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bio
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CV
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            CV
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Web Format
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../documents/cv_full.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDF
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="./" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content" data-md-component="content">
    <div class="md-content__inner">
      <header class="md-typeset">
        <h1 id="blog">Blog</h1>
      </header>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-06-18 00:00:00">June 18, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="category/ai/" class="md-meta__link">AI</a>, 
              <a href="category/llms/" class="md-meta__link">LLMs</a>, 
              <a href="category/writing/" class="md-meta__link">Writing</a>, 
              <a href="category/coding/" class="md-meta__link">Coding</a></li>
        
        
          
          <li class="md-meta__item">
            
              20 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="will-ai-replace-us-the-role-of-llms-in-writing-and-coding"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/">Will AI Replace Us? The Role of LLMs in Writing and Coding</a></h2>
<h3 id="introduction"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#introduction">Introduction</a></h3>
<p>The emergence of ChatGPT in 2022 and the widespread use of intelligent chat bots have increased the popularity of Language Models (LM) [1], these Artificial Intelligence (AI) systems have not only transformed how we communicate but also have started to change key sectors including research, education software development and the arts [2]. Moreover, their ability to generate and understand human text has opened up new opportunities for development of automation and creativity across disciplines [2, 3].</p>
<p>The integration of LMs into these worlds, however, has brought a series of trials and ethical issues that are not just technical [4]. In research writing, for instance, LLMs are being used to help compose papers [2]. This has prompted questions concerning facile authorship, authenticity, and whether these AI systems must be made responsible for revising scientific manuscripts instead of human reviewers. Meanwhile, software engineering tools like GitHub Copilot [5] allow coding in a natural language to be translated into machine code, but they also introduce concerns regarding privacy, cybersecurity, and reliance on these automated tools.</p>
<p>The creative industries also are experiencing a similar issue, on the one hand, LMs can collaborate with artists and writers to generate new of literature, music and visual art. While, parallel to research writing, the widespread use of these technologies in creative sectors raises questions about where the limits of creativity and authenticity lie. Who is truly developing new art-the human or the machine? If the latter, can such content be considered original? These concerns are amplified by the fact that LMs are trained using vast datasets, including works by renowned authors.</p>
<p>This essay seeks to delve into these debates to discuss the role of current LMs in our society and whether, as automated tools, they might eventually replace us in the near future, especially in the contexts of research and software development. By analyzing both the opportunities and the risks associated with LLMs, it aims to analysis how these models are shaping the future of knowledge creation, technological innovation, and creative expression. Consequently, this report is organized as follows. First, Section 2 explains what LMs are from a technical perspective, why they have been a breakthrough in recent years, and whether they have the potential to replace us in the future. Second, Section 3 discusses how LMs have impacted academic writing and its ethical debates. Third, in Section 4, it explores the adoption of LMs in the software engineering industry and their potential to replace programmers. Finally, in Section 5, I provide final insights on these questions and discuss what we can expect from the future development of AI in these areas.</p>
<h3 id="ai-and-large-language-models"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#ai-and-large-language-models">AI and Large Language Models</a></h3>
<p>The ability to master human language have been always a goal in Computer Sciences. In 1950, Alan Turing developed his famous "Turing Test" [6], which determines whether machines can hold a conversation with human users without the users realizing they are interacting with a machine. More than ever, the Turing Test continues to challenge the boundaries of computer science and artificial intelligence.</p>
<p>In 1964, Joseph Weizenbaum from MIT introduced ELIZA [7], one of the first conversational agents in the history of Computer Science and AI. ELIZA sought to simulate a psychotherapist by using a set of logical rules to interact with human users, giving the illusion of text understanding even though it was simply responding to certain word patterns with predefined rules. While, in the 1950s, Claude Shannon pioneered statistical and predictive modeling of written language [8]. Using information theory, he measured the difficulty of predicting words based on previous context in a text corpus, laying the groundwork for later statistical language models. However, in 1957, linguist Noam Chomsky criticized the limitations of purely statistical models for capturing the complexities of human grammar [9]. To demonstrate this, he presented two invented sentences: "Colorless green ideas sleep furiously" and "Furiously sleep ideas green colorless". Chomsky argued that even though both lack of semantic meaning, just the first one it is syntactically correct, but the statistical LMs study by Shannon would consider both sentences equably plausible.</p>
<p>The development of LMs can be divided into three phases, which I will explain in the following subsections:</p>
<h4 id="statistical-language-models"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#statistical-language-models">Statistical Language Models</a></h4>
<p>In 1990, there was a significant breakthrough in how language was processed by computer systems, introducing the idea of using probabilities to compute the likelihood of each possible sentence from a finite set of words \cite{liu2005statistical}. For example, the following sentences:</p>
<ul>
<li>The dog barks.</li>
<li>A bird flies to the ground.</li>
</ul>
<p>Both of these sentences are syntactically correct because they follow the rules of the English language, but only the first one is semantically correct. It is common knowledge that dogs are able to bark, but it does not make sense that birds fly to the ground instead of the sky. Consequently, it is expected that a language model could assign a higher probability to the first sentence.</p>
<p>From a mathematical perspective, each word, $w$, is treated as a random discrete variable from a finite set of words called the vocabulary. Therefore, the model can assign a probability to any possible sentence:</p>
<p>$$ p(s) = p(w_1, w_2, ..., w_n) $$</p>
<p>This formula can be converted to this using mathematical properties and compute all of the probability related to a given sentence $w_1, w_2, ..., w_n$:</p>
<p>$$ p(w_1, w_2, ..., w_n) = p(w_1) \times  p(w_2|w_1) \times p(w_3|w_2,w_1) \times ... \times p(w_n|w_1, ..., w_{n-1})$$</p>
<p>However, computing the probabilities of this formula can be really computationally expensive when a long sentence is found in a corpus of text. To address this limitation, researchers have used the Markov assumption, which restricts the amount of memory of previous words needed to predict the next word. Reducing the formula to this:</p>
<p>$$p(w_1, w_2, ..., w_n) = p(w_1) \times p(w_2|w_1) \times p(w_3|w_2) \times ... \times p(w_n|w_{n-1})$$</p>
<p>With these changes, processing and training statistical language models become more tractable using computer resources. In Figure is possible to visualize how probability can be scored. However, these models present several limitations, including difficulty in processing long contexts and an inability to detect similar textual contexts.</p>
<p>\begin{figure<em>}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/HMMGraph.svg.png}
  \caption{\textbf{HMM example from Wikipedia.}}
  \label{fig:hmm}
\end{figure</em>}</p>
<h4 id="neural-language-models"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#neural-language-models">Neural Language Models</a></h4>
<p>Neural Language Models (NLMs) \cite{jing2019survey} introduce a paradigm shift against stadistical LMs to face their limitation such issues to process long sentences and long texual contexts. These models learn distributed representations of words (called word embeddings \cite{goldberg2014word2vec}) and predict the probability distribution of the next word in a sequence of words or sentence. Unlike statistical LMs that rely on explicit counts of word combinations, NLMs learn latent features and capture patterns from from data.</p>
<h4 id="word-embeddings"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#word-embeddings">Word Embeddings</a></h4>
<p>Word embeddings are dense, low-dimensional vector representations of words that capture semantic (meaning-based) and syntactic (grammar-based) relationships \cite{goldberg2014word2vec}. Consequently, these model map words to continuous vectors where similar words are closer in the vector space.</p>
<p>The benefit of this approach is that the vector space, based on the vocabulary of the text corpus, allows us to mathematically measure whether two words are similar using properties such as distance functions, including cosine similarity. For example, the figure shows that the word vectors capture both syntactic and semantic similarities in the language.</p>
<p>\begin{figure<em>}[h]
  \centering
  \includegraphics[width=.9\textwidth]{images/we.png}
  \caption{\textbf{Vectorial space built by Word Embeddings.}}
  \label{fig:hmm}
\end{figure</em>}</p>
<p>However, Word Embeddings present two important limitations, which </p>
<ul>
<li>Word Embeddings are static vector representation that assign the same vector to a word regardless of its context, conflating multiple meanings (polysemy). For example, the word bank can be used in language as bank institution, but at the same time as river edge.</li>
<li>Word Embeddings struggle to handle rare words or Out-of-vocabulary (OOV) words (words not seen during training get random vectors), assigning not good representations to these words.</li>
</ul>
<h4 id="recurrent-neural-networks-in-language-modeling"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#recurrent-neural-networks-in-language-modeling">Recurrent Neural Networks in Language Modeling</a></h4>
<p>Recurrent Neural Networks (RNNs) are a specialized type of neural network designed to process sequential data, such as text, where the order of elements is crucial \cite{mikolov2010recurrent}. In language modeling, RNNs are used to predict the probability of the next word or character in a sequence, given the preceding context.</p>
<p>RNNs maintain a hidden state that acts as memory, capturing information from previous time steps. At each step, the RNN takes the current input (e.g., a word or character) and the previous hidden state to produce a new hidden state and an output. This allows the network to "remember" past information and use it to inform future predictions. On the other hand, RNNs mitigate Word Embeddings limitation using character-level or sub-word units to build word representations from smaller, shared components allowing to learn different representations with words that have several meaning based on the context.</p>
<p>Although NLMs represent an advancement over statistical language models, these models could only address one task per architecture, and were not capable of generalizing to other tasks, unless the approach or the architecture, for which the model was initially designed was changed.</p>
<h4 id="pre-trained-language-models"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#pre-trained-language-models">Pre-trained Language Models</a></h4>
<p>One limitation with NLMs and with neural network-based models in general is that they require a massive amount of data to achieve good performance on NLP tasks, including sentiment classification, text summarization, or question answering. However, the specific task that LMs face is to predict the next word in a sentence given the previous ones; this approach is based on the distributional hypothesis. For example, in the sentence ``I have a gift for my ...'', an LM must determine which word is plausible according to the probabilities of the words in the vocabulary and the context.</p>
<p>According to the task of predicting the word in sentence Peters et al. in 2018 advanced this idea to the next level and propose ELMO \cite{peters-etal-2018-deep}. ELMO is a LMs based on recurrent neural networks trained on massive corpus of text. The main idea of ELMo, unlike other models based on recurrent neural networks, was to leverage the knowledge acquired from vector representations of text to predict the next word in a sentence, which proposes a paradigm shift by using not only the previous words and context to predict the next word, but also the knowledge acquired by the model during its training phase. While this achievement surpassed several state-of-the-art models at the time, ELMo required processing each word in a sentence sequentially, which did not allow for parallelization of this task and made training for scalability with larger datasets more difficult.</p>
<p>To address these challenges, Vaswani et al. \cite{vaswani2017attention} in 2017 introduced a new neural network architecture called the Transformer, based on the attention mechanism, which enables parallel processing of text sequences. This makes them particularly well-suited for execution on specialized hardware such as GPUs and TPUs, which, in turn, makes them highly parallelizable. These achievements paved the way for the introduction of pre-trained language models \cite{min2023recent}, which were no longer trained from scratch but were instead pre-trained on the language modeling task. This knowledge could then be leveraged for more specific tasks, such as sentiment classification.</p>
<p>Companies like OpenAI\footnote{https://openai.com/} leverage these advances to scale and develop their own Transformer-based models, known as Generative Pretrained Transformers (GPT) \cite{yenduri2024gpt}. This series includes GPT-1, GPT-2, and GPT-3, each with a larger parameter size than its predecessor. However, the most notable aspect of these models is the emergent properties that manifested as parameter size increased. These properties open a difficult debate among the academic community about whether these emergent properties truly represent context-based learning, or if they are simply a manifestation of the extensive training corpus that contain relevant information for each existing tasks in the SOTA. </p>
<p>The emergent properties in LMs \cite{wei2022emergent}, now called Large LMs due to the vast number of parameters in their neural-based architectures, have allowed them to solve a wide range of tasks, such as question answering or mathematical problems, as long as these tasks can be expressed in text. This capability has given rise to what is now known as Prompt Engineering.</p>
<p>All these advancements have converged in what we now know as the generative capabilities of LLMs (the ability to generate text at a human level) \cite{feuerriegel2024generative}, which have sparked extensive debates about whether these AI systems will replace us in the near future.</p>
<h3 id="role-of-llms-in-research-writing"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#role-of-llms-in-research-writing">Role of LLMs in Research Writing</a></h3>
<p>In March 2024, a tweet\footnote{https://x.com/gcabanac/status/1767574447337124290?s=20} made viral in social media regarding a paper published in the academic journal  Elsevier’s Surfaces and Interfaces\footnote{https://www.sciencedirect.com/journal/surfaces-and-interfaces} because its introduction started with the following sentence: ``Certainly, here is a possible introduction for your topic'', which is a typical answer from ChatGPT to user questions. LLMs such as ChatGPT and others have been increasingly used to assist academic writing, showing rapid adoption in computer science papers. For example, Zou et al. \cite{liang2024monitoring} found that 17.5\% of computer science papers and 16.9\% of peer review text had at least some content drafted by AI. The paper on LLM usage in peer reviews will be presented at the International Conference on Machine Learning (ICLM). To illustrate these findings, Zou et al. \cite{liang2024mapping} highlight specific words such as commendable, innovative, meticulous, pivotal, intricate, realm, and showcasing which are more commonly used in ChatGPT-generated answers than in human writing, by comparing several papers published before and after the release of ChatGPT.</p>
<p>This has become a problem for journals and scientific conferences, as they do not know how to manage the use of AI tools. Some journals have forbidden their use in calls for papers, while others have allowed their use as long as authors explicitly state their use in the contributions of each author. However, the big question is: is the use of AI tools for improving academic writing an ethics violation? On the one hand, since most research literature is written primarily in English, using these tools can help facilitate and level the playing field for researchers who are not native English speakers. On the other hand, LLMs have been trained on large data repositories, often without proper concern for copyright, raising questions about the originality of the content generated when using LLMs in scientific writing. Furthermore, a question that the scientific community has been asking is whether LLMs are capable of demonstrating basic reasoning abilities. What would happen if LLMs were scaled even further, made even larger? Would they achieve higher levels of reasoning, or are these models simply capturing statistical patterns, generating the most likely outputs based on internal probabilities? </p>
<p>From the perspective of the author of this article, LLMs fall into the latter category, tools that can very effectively mimic human patterns at scale. This would imply that these models can learn to imitate, but not to generate new knowledge. Can we entrust the development of science to such an AI? Clearly not. Even more so, can we trust it with the dissemination of science? A resounding no. Therefore, scientific research must still be carried out by humans, especially its dissemination through academic writing.</p>
<p>Another topic that has been widely debated in the scientific community is the use of LLMs for reviewing scientific articles \cite{zhou2024llm}. Should these models be used instead of the traditional peer review process in journals and scientific conferences? A major issue in academia is that researchers are often very busy working on their own research, leaving them with little time to focus on tasks unrelated to their main projects. As a result, the use of LLMs has become widespread among researchers for reviewing scientific papers. However, as mentioned earlier, LLMs are not capable of identifying the gaps that a paper might have, making human review by an expert in the field essential. Furthermore, many LLMs are general-purpose, while reviewing scientific articles requires a high level of specialization that LLMs often lack within their internal knowledge.</p>
<p>However, from my perspective as the author of this article, I do agree with the use of LLMs as complementary tools that facilitate the research process. For example, using LLMs to help correct the English grammar of non-native speakers, or to summarize papers to make them easier to read. Furthermore, some articles can be difficult to understand when the reader is not an expert in the field. The use of LLMs can help mitigate this by making such papers more accessible and by suggesting new directions for current research. Furthermore, during the article review process, in the opinion of this author, LLMs can also assist reviewers by pointing out potential aspects that might be overlooked. However, under no circumstances should they replace researchers in the traditional peer review process.</p>
<h3 id="coding-and-software-development"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#coding-and-software-development">Coding and Software Development</a></h3>
<p>LLMs have transformed the way software engineering is carried out across different development teams, bringing several technical strengths and benefits that improve productivity and code quality \cite{kirova2024software}. For example, the generative properties of LLMs excel at improving code generation and automating documentation tasks. Consequently, developers have started to use these tools as part of their daily work, among the technical benefits that LLMs have brought:</p>
<ul>
<li><strong>Code Generation</strong>: LLMs like GitHub Copilot, equipped with OpenAI's Codex, enable developers to generate code from natural language descriptions, which speeds up coding and reduces repetitive tasks. This increases productivity by automating routine activities such as writing code and implementing algorithms. Furthermore, LLMs offer cross-language support, allowing developers to work across multiple programming languages without needing to master each one and opening up access to write code from multiple programming languages using just one tool.</li>
<li><strong>Code review and Debugging</strong>: LLMs are used to automate code reviews and assist with bug detection, changing the software development process and enhancing code quality. For example, models such as ChatGPT can analyze code for errors, suggest improvements, and help junior developers perform effective code reviews by leveraging  best practices. Traditionally, debugging is a manual process, but LLMs can analyze logs, error messages, and  suggest potential fixes. This capability is especially valuable in large systems, where bugs are difficult to track down and fix.</li>
<li><strong>Refactoring</strong>: As software systems grow, they accumulate technical debt, which makes software systems difficult to maintain in the long-term and might affect their performance in the future. LLMs can help by identifying code that needs refactoring, suggesting simplifications and eliminating unnecessary code. They also recommend more efficient algorithms and design patterns, and provide insights into performance issues such as inefficient loops or memory usage.</li>
<li><strong>Documentation</strong>: Keeping software documentation accurate is challenging and time-consuming, but its importance for maintainability and collaboration is one of the most important parts of software development workflow. LLMs can automatically generate and update documentation by analyzing code, explaining functions, classes, and modules. This keeps documentation aligned with code changes and helps developers understand complex, especially in agile environments where requirements frequently change.</li>
</ul>
<p>These are some of the main technical benefits that current LLMs have brought to software development teams. However, these benefits have also raised a series of concerns among researchers and tech leads regarding the use of LLMs by developers and software engineering teams. Which from a technical perspective LLMs are far from being perfect, for example of them limitation that they have:</p>
<ul>
<li><strong>Lack of code comprehension</strong>: LLMs lack human-like comprehension of code, they rely on statistical and probabilities patterns to predict sequences rather than understanding underlying the intent. This poses challenges in software engineering, where code must align with business requirements and software best practices. For example, an LLM might generate a syntactically correct sorting algorithm but overlook implicit efficiency needs like time complexity. Consequently, while LLMs aid in generating code snippets, their outputs require human review to ensure they meet functional and non-functional system requirements.</li>
<li><strong>Inability to handle rare issues</strong>: LLMs like GitHub Copilot are really good at learning patterns from vast text information, which means they perform best when tasked with solving common or well-documented problems. However, when faced with problems that are not present in their training data, LLMs struggle to find effective solutions or generate meaningful answers for novel issues that arise from changing client requirements or project specifications in software engineering. </li>
<li><strong>Computational resources</strong>: LLMs require a lot of computational resources for both training and inference, usually this rely on large clusters of GPUs or TPUs, which leads to high financial costs and resource-intensive operations. This high demand for requirements may prevent small companies from investing in their infrastructure and incorporating them into their software engineering workflow, unlike large tech companies that have the financial resources to integrate these technologies. Additionally, both the training and deployment of LLMs consume large amounts of energy, contributing to environmental impacts such as higher carbon emissions and greater energy usage.</li>
<li><strong>Black-box systems</strong>: A major issue with LLMs is that they are black-box systems, and access to these models is often only available through paid APIs provided by large tech companies like OpenAI and Google. Moreover, even with complete access to the models and their code, it is not possible to fully understand how they generate answers. This is because the complex operations performed by LLMs require sophisticated techniques to interpret their behavior.</li>
<li><strong>Security issues</strong>: Building LLMs on public code repositories is a potential hazard, given that if those errors exist in the training data as insecure practices such as hard-coded credentials and weak encryption methods, the models of LLM will mimic these vulnerabilities as well \cite{sallou2024breaking}. As explained earlier, LLMs are good at capturing patterns; consequently, if vulnerabilities are present in their training data, the models will learn representations based on them. This becomes an issue if developers use LLMs to write code, as the generated code may contain vulnerabilities and insecure practices.</li>
</ul>
<p>Some technology experts, such as the CEO of NVIDIA, claim that programmers will no longer be necessary because LLMs will be capable of reaching AGI (Artificial General Intelligence) within a few years\footnote{https://www.tomshardware.com/tech-industry/artificial-intelligence/jensen-huang-advises-against-learning-to-code-leave-it-up-to-ai}, possessing all the knowledge needed to replace us—especially in automated tasks like software programming. Although, many of the issues mentioned in the previous points are the challenges that LLMs must overcome before they can operate without human supervision. Moreover, since LLMs operate based on statistical patterns derived from data, it will be impossible to completely overcome these issues, though they can be mitigated using more sophisticated techniques. Therefore, human supervision from programmers and developers will still be required.</p>
<h3 id="conclusions"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#conclusions">Conclusions</a></h3>
<p>The emergence of LLMs, which are revolutionizing many fields such as research and the software industry, is the result of years of research and advancements in the field of NLP. The first language models were developed based on capturing statistical patterns in training data using Markovian assumptions. Later, neural networks were introduced to better encode these patterns and leverage this knowledge for other tasks, marking a paradigm shift with the advent of pre-trained models. Pre-trained models enabled the development of AI systems capable of retaining certain knowledge without needing to be trained from scratch. The arrival of the Transformer architecture allowed these models to be trained at scale, making better use of computational resources like GPUs and TPUs, giving rise to what we now know as LLMs. </p>
<p>Given the capabilities of LLMs to process and understand text, they have become useful tools in many fields, such as scientific writing and software engineering, even raising the question of whether these AI systems might replace humans in such tasks in the near future. On the one hand, LLMs help facilitate writing in English for researchers who are not native speakers, and can assist in the article review process by pointing out aspects a reviewer might overlook. However, LLMs face a number of challenges that must be overcome before they can replace researchers in these tasks. For example, while LLMs are good at mimicking patterns, they are not able to propose new research directions based on current gaps in the literature. 
On the other hand, in software engineering, LLMs are incredibly good at understanding coding problems and generating possible solutions. This capability allows LLMs to act as assistants that can generate documentation, suggest best coding practices, and refactor code according to the developer’s specifications. However, as with the case of scientific writing, when LLMs face unfamiliar or novel problems, they are often unable to solve them satisfactorily because such problems are not present in the data they were trained on. As a result, current LLMs are not capable of extrapolating complex solutions for new or unseen problems, unlike humans. Additionally, since LLMs learn patterns at a massive scale, they can absorb both good and bad practices. For example, they might learn poor programming habits or suggest inefficient algorithms for problems that require more optimized solutions.</p>
<p>Therefore, given the challenges of current LLMs, they are still far from replacing humans in scientific writing and software engineering. However, this does not mean that these models cannot help accelerate the automation of repetitive tasks, such as generating similar code or documentation. As a result, significant changes are expected in how researchers and developers carry out their work when equipped with these tools. Will they only bring good changes? It is hard to say. In the context of software engineering (or the industry in general), some job positions might no longer be needed. But for now, LLMs are still assistants, not researchers or developers.</p>
<h3 id="references"><a class="toclink" href="2025/06/18/will-ai-replace-us-the-role-of-llms-in-writing-and-coding/#references">References</a></h3>
<p>[1] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muham-
mad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on large language
models: Applications, challenges, limitations, and practical usage. Authorea Preprints, 3, 2023.</p>
<p>[2] Chanlang Ki Bareh. A qualitative assessment of the accuracy of ai-llm in academic research. AI and
Ethics, pages 1–20, 2025.</p>
<p>[3] Vassilka D Kirova, Cyril S Ku, Joseph R Laracy, and Thomas J Marlowe. Software engineering
education must adapt and evolve for an llm environment. In Proceedings of the 55th ACM Technical
Symposium on Computer Science Education V. 1, pages 666–672, 2024.</p>
<p>[4] Atte Laakso, Kai-Kristian Kemell, and Jukka K Nurminen. Ethical issues in large language models: A
systematic literature review. In CEUR Workshop Proceedings, volume 3901, pages 42–66. CEUR-WS,
2024.</p>
<p>[5] Michel Wermelinger. Using github copilot to solve simple programming problems. In Proceedings of
the 54th ACM Technical Symposium on Computer Science Education V. 1, pages 172–178, 2023.</p>
<p>[6] James Moor. The Turing test: the elusive standard of artificial intelligence, volume 30. Springer
Science &amp; Business Media, 2003.</p>
<p>[7] David M Berry. The limits of computation: Joseph weizenbaum and the eliza chatbot. Weizenbaum
Journal of the Digital Society, 3(3), 2023.</p>
<p>[8] Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50–
64, 1951.</p>
<p>[9] Vivian Cook. Chomsky’s syntactic structures fifty years on. International Journal of Applied Lin-
guistics, 17(1):120–131, 2007.</p>
<p>[10] Xiaoyong Liu and W Bruce Croft. Statistical language modeling for information retrieval. Annu. Rev.
Inf. Sci. Technol., 39(1):1–31, 2005.</p>
<p>[11] Kun Jing and Jungang Xu. A survey on neural network language models. arXiv preprint
arXiv:1906.03591, 2019.</p>
<p>[12] Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.’s negative-sampling
word-embedding method. arXiv preprint arXiv:1402.3722, 2014.</p>
<p>[13] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010.</p>
<p>[14] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn Walker, Heng Ji, and
Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),
pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.</p>
<p>[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.</p>
<p>[16] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,
Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large
pre-trained language models: A survey. ACM Computing Surveys, 56(2):1–40, 2023.</p>
<p>[17] Gokul Yenduri, M Ramalingam, G Chemmalar Selvi, Y Supriya, Gautam Srivastava, Praveen Ku-
mar Reddy Maddikunta, G Deepti Raj, Rutvij H Jhaveri, B Prabadevi, Weizheng Wang, et al. Gpt
(generative pre-trained transformer)–a comprehensive review on enabling technologies, potential ap-
plications, emerging challenges, and future directions. IEEE Access, 2024.</p>
<p>[18] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682, 2022.</p>
<p>[19] Stefan Feuerriegel, Jochen Hartmann, Christian Janiesch, and Patrick Zschech. Generative ai. Busi-
ness &amp; Information Systems Engineering, 66(1):111–126, 2024.</p>
<p>[20] Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao
Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. Monitoring ai-modified content at scale: A case study
on the impact of chatgpt on ai conference peer reviews. arXiv preprint arXiv:2403.07183, 2024.</p>
<p>[21] Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng
Cao, Sheng Liu, Siyu He, Zhi Huang, et al. Mapping the increasing use of llms in scientific papers.
arXiv preprint arXiv:2404.01268, 2024.</p>
<p>[22] Ruiyang Zhou, Lu Chen, and Kai Yu. Is llm a reliable reviewer? a comprehensive evaluation of
llm on automatic paper reviewing tasks. In Proceedings of the 2024 Joint International Conference
on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages
9340–9351, 2024.</p>
<p>[23] June Sallou, Thomas Durieux, and Annibale Panichella. Breaking the silence: the threats of using
llms in software engineering. In Proceedings of the 2024 ACM/IEEE 44th International Conference
on Software Engineering: New Ideas and Emerging Results, pages 102–106, 2024.</p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-04-25 00:00:00">April 25, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="category/tips/" class="md-meta__link">Tips</a>, 
              <a href="category/phd/" class="md-meta__link">PhD</a></li>
        
        
          
          <li class="md-meta__item">
            
              3 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="being-a-phd-in-computer-science-in-norway"><a class="toclink" href="2025/04/25/being-a-phd-in-computer-science-in-norway/">Being a PhD in Computer Science in Norway</a></h2>
<p>It has been nearly six months since I began my PhD position at the University of Stavanger in Norway, and it has also been quite some time since my last blog post. Therefore, I decided to write this post to explain how I got my PhD position and the reasons behind my decision to come to such a beautiful country as Norway. Also, this is my first post in English, so some expressions or ideas might not be perfectly written. If something is unclear, please let me know, I am always interested in improving my English :)</p>
<p>It is also relevant to highlight that this post is primarily intended for students who are interested in pursuing a PhD in Computer Science or Artificial Intelligence (AI), which are the fields I am currently researching. However, some of the points I mention here may also apply to other areas. Consequently, if you are looking to pursue research in other specific fields in Norway, this post could still be relevant for you :D</p>
<p>Before getting to the important point and probably the reason you clicked on this article-you need to ask yourself an important question: <strong>Do you really want to pursue a PhD?</strong> <strong>Are you sure?</strong> Why am I asking you this? Because after working here for six months, I must admit it has not been an easy task. You must understand that doing a PhD requires strong mental willpower. You will be working on a research project, yes, RESEARCH, not engineering for at least three or four years, and you will probably have to leave many things behind, including your job, family, friends, and your entire life in your home country. So, if you still want to pursue a PhD after all these warnings, keep reading.</p>
<h3 id="why-you-must-choose-norway-for-doing-you-phd"><a class="toclink" href="2025/04/25/being-a-phd-in-computer-science-in-norway/#why-you-must-choose-norway-for-doing-you-phd">Why you must choose Norway for doing you PhD?</a></h3>
<p>In my case, I must say that I didn’t come here for the reasons I will outline below-my situation was a bit different, but it’s not relevant. So, what are the reasons you should choose Norway for your PhD?</p>
<p>You are not a student, you are employee: As a PhD candidate in Norway, you are considered a full-time employee rather than just a student, which means access to several benefits that other countries do not provide for PhD students, including:</p>
<ul>
<li>Employment Contract: You have a formal employment contract, which outlines your rights and duties, just like any other employee in Norway</li>
<li>
<p>Social Benefits: </p>
<ul>
<li>
<p>Paid Holidays: You have the right to paid holidays, following the same regulations as other employees in Norway (typically five weeks per year).</p>
</li>
<li>
<p>Sick Leave and Parental Leave: You have access to sick leave and parental leave on the same terms as other Norwegian employees.</p>
</li>
<li>
<p>Pension access: Membership in the Norwegian Public Service Pension Fund is mandatory, giving you access to retirement and disability pensions, as well as favorable housing loan conditions.</p>
</li>
</ul>
</li>
<li>
<p>Health Care System: As an employee, you have full access to Norway’s public health care system. In other countries, such as Australia, you must get a health insurance to cover the entire period you are working on your PhD, which actually it is not cheap.</p>
</li>
</ul>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-03-03 00:00:00">March 3, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="category/tips/" class="md-meta__link">Tips</a>, 
              <a href="category/phd/" class="md-meta__link">PhD</a></li>
        
        
          
          <li class="md-meta__item">
            
              6 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="tips-para-buscar-phd-en-el-extranjero"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/">Tips para buscar PhD en el extranjero</a></h2>
<p>Hace unos meses, me encontraba en la activa búsqueda de programas de Doctorado para continuar mis estudios y no sabía por dónde empezar. A pesar de la abundancia de información en Internet, nunca logré encontrar un blog con tips especificos que se ajustaran a mi realidad. Por esto, he decidido compartir mi experiencia y compartir algunos consejos que me fueron útiles en la búsqueda de un programa de doctorado en el extranjero.</p>
<p>Antes de comenzar, quiero señalar algunos puntos importantes. En primer lugar, soy consciente de que mi experiencia no es universal; es posible que lo que me funcionó a mí no se aplique a todos. Además, hay muchas otras maneras de abordar esta búsqueda, más allá de las que compartiré aquí. En segundo lugar, debo aclarar que no soy doctor ni estudiante de doctorado. Es más, actualmente, trabajo en una startup de Inteligencia Artificial. Sin embargo, logré conseguir algunas entrevistas exitosas. En tercer lugar, mis consejos estarán principalmente orientados a la búsqueda de estudios de posgrado en Ciencias de la Computación e Inteligencia Artificial, dado que esta es mi área de formación. Aunque creo que estos consejos generales pueden ser de utilidad en otras áreas, los detalles más específicos se centrarán en las disciplinas de Ciencias de la Ingeniería.</p>
<h3 id="porque-seguir-estudios-de-doctorado"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#porque-seguir-estudios-de-doctorado">¿Porqué seguir estudios de Doctorado?</a></h3>
<p>Esta es una pregunta crucial que una persona debe ser capaz de responder al comenzar la búsqueda de estudios de postgrado, ya sea de maestría o doctorado. Esto se debe a que, aunque estudiar en otro país puede ser la mejor experiencia de tu vida, existen numerosos riesgos que deberás enfrentar. Entre ellos se incluyen dejar a tu familia y amigos, aprender nuevos idiomas (ya que el inglés podría servir para comunicarse en el ámbito universitario, pero no necesariamente en el día a día de una cultura diferente, donde las personas no tienen por qué saber hablar inglés para comunicarse con extranjeros). Otro punto a considerar es la situación financiera; seguir un programa de maestría o doctorado significa que posiblemente no podrás trabajar durante varios años. Y, aunque las becas proveen para vivir durante ese tiempo, es mucho menos dinero que el salario que podrías obtener en la industria. Por estas y otras razones, responder a esta pregunta es fundamental: ¿Por qué quieres estudiar una Maestría o un Doctorado? Sea cual sea tu motivación, debe ser firme, ya que, aunque podría ser la mejor aventura de tu vida, no quita que también podría ser una de las más difíciles. Si aún no sabes la respuesta a esta pregunta, dejo algunos enlaces que podrían ayudarte a encontrarla:</p>
<ul>
<li><a href="https://www.linkedin.com/pulse/4-compelling-reasons-why-pursuing-phd-worth-globalresearchnetwork/">4 Compelling Reasons Why Pursuing a PhD is Worth It</a></li>
<li><a href="https://www.princetonreview.com/grad-school-advice/why-you-shouldnt-pursue-phd">Why You Shouldn’t Get a PhD</a></li>
</ul>
<p>De cualquier manera, no quiero desmotivar ni asustar a nadie. Si bien es cierto que pueden existir días difíciles durante tus estudios, lo importante es creer en uno mismo y nunca rendirse. La vida es corta y los sueños están para cumplirse. Así que, si estás pensando o dudando por las dificultades que podrían surgir, no te preocupes; si has llegado hasta aquí, ya sea para una maestría o un doctorado, puedes lograrlo.</p>
<h3 id="por-donde-empezar"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#por-donde-empezar">¿Por donde empezar?</a></h3>
<p>Creo que esta es la parte más importante de este blog. Por cierto, si han llegado hasta aquí, ¡muchas gracias por leer! 😊</p>
<p>Continuando, una pregunta importante es cómo empezar a buscar. ¿Es mejor comenzar por la universidad o el supervisor?, o quizás, ¿el lugar, por ejemplo, el país de destino? Pues la verdad, ninguna de estas opciones es el mejor punto de partida. Lo más recomendable es comenzar por la disciplina. Y cuando digo disciplina, no me refiero a algo general como investigar en IA, sino a los subcampos que puede englobar. Por ejemplo, en mi caso personal, inicialmente estaba muy interesado en investigar sobre Procesamiento de Lenguaje Natural (NLP) y Recuperación de Información (IR), ya que fue lo que investigué durante mi maestría y era lo que más conocía. Sin embargo, después de leer muchos artículos sobre el tema y hablar con más personas, decidí que me interesaba más investigar sobre Aprendizaje Reforzado (RL). Aunque es una línea diferente a lo que ya había hecho, creo que es fundamental ser exigente y honesto con lo que TÚ realmente quieres hacer, ya que pasarás al menos de 2 a 4 años investigando y trabajando en algo que podría definir tu futuro.</p>
<p>Luego de definir la disciplina que quieres aprender e investigar, el siguiente paso más importante es definir el tema o tópico; me refiero al potencial tema sobre el que trabajarás en tu tesis de maestría o doctorado. Alguien podría argumentar, ¿pero por qué sería necesario tener esto tan claro desde el principio? ¿No podría simplemente hablarlo con mi futuro supervisor? Eso es cierto. Sin embargo, cuanto más claras tengas estas dos cosas como futuro estudiante, mayor ventaja tendrás sobre el resto, ya que podrás focalizar tu búsqueda en supervisores que trabajen activamente en la disciplina y tópico que hayas escogido. Créeme, esto facilitará mucho la búsqueda por dos razones. Primero, podrás filtrar a los supervisores que trabajen en dichas áreas y tópicos, y segundo, mientras más claras tengas tus ideas, será más fácil conseguir una respuesta de ellos.</p>
<h3 id="como-empezar-a-buscar-y-financiamiento"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#como-empezar-a-buscar-y-financiamiento">¿Cómo empezar a buscar y financiamiento?</a></h3>
<p>Luego de haber decidido la disciplina y el tópico, surge la pregunta de cómo empezar a buscar un potencial supervisor en mis áreas de interés. En mi experiencia, identifiqué cuatro formas en las que pude, al menos, enviar mis antecedentes a varios potenciales supervisores:</p>
<ul>
<li>Buscar supervisores de paper interesantes que hayan leído.</li>
<li>Páginas de anuncios.</li>
<li>Proyectos de investigación en institutos y/o universidades.</li>
<li>Utilizando redes sociales.</li>
</ul>
<h4 id="potenciales-supervisores-a-traves-de-articulos-interesantes"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#potenciales-supervisores-a-traves-de-articulos-interesantes">Potenciales supervisores a través de articulos interesantes.</a></h4>
<p>Quizás sea el más obvio de todos, pero buscar supervisores dispuestos a investigar en tus áreas de interés a través de artículos que ellos mismos hayan publicado es un gran punto de partida para iniciar una conversación con ellos. Para esto, hay muchas páginas desde donde comenzar a buscar, pero la más simple, en mi opinión, es <a href="https://scholar.google.com/">Google Scholar</a> ya que allí pueden encontrar artículos interesantes a través de palabras claves, de la misma manera que utilizamos el buscador normal de Google. Por ejemplo, si desean investigar sobre Visión por Computadora como disciplina y análisis de imágenes usando Deep Learning como tópico, pueden utilizar las palabras clave "Computer Vision" y "Deep Learning". Luego, aparecerán una serie de artículos científicos que podrían ser de su interés, en los cuales identificar potenciales supervisores. Posteriormente, pueden buscar en Google el nombre del investigador o investigadora y encontrar mayor información sobre ellos y conocer más de su trabajo. Generalmente, los investigadores poseen páginas personales donde muestran su trabajo o perfiles en las universidades donde trabajan.</p>
<p>Sin embargo, un considerable obstáculo es que, aunque estos supervisores puedan estar dispuestos a trabajar en un proyecto de tesis, es posible que no dispongan de fuentes de financiamiento. Esto podría impedir que sean capaces de supervisar tu trabajo adecuadamente. No obstante, en el contexto de Chile, existe el programa de becas Chile, que ofrece la posibilidad de obtener financiamiento para estudios a través de una postulación rigurosa. Por lo tanto, es crucial mencionar este aspecto cuando contactes a un potencial supervisor.</p>
<h4 id="paginas-de-anuncios"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#paginas-de-anuncios">Páginas de anuncios</a></h4>
<p>A través de Internet, existen muchas páginas de anuncios donde los investigadores ofrecen proyectos de tesis (principalmente de doctorado) con una descripción detallada del laboratorio donde trabajan, el proyecto y las fuentes de financiamiento. Las que más recomiendo son:</p>
<ul>
<li><a href="">https://www.findaphd.com/</a></li>
<li><a href="">https://academicpositions.es/jobs/position/phd</a></li>
<li><a href="">http://scholarshipdb.net/</a></li>
</ul>
<p>Cada una de ellas cuenta con filtros principales que facilitan la búsqueda.</p>
<h4 id="proyectos-de-investigacion-en-institutos-yo-universidades"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#proyectos-de-investigacion-en-institutos-yo-universidades">Proyectos de investigación en institutos y/o universidades</a></h4>
<p>Muchas universidades e institutos disponen de varios proyectos de investigación liderados por alguno de sus académicos. Al igual que en el punto anterior, estos proyectos cuentan con una descripción detallada del laboratorio donde trabajan, el proyecto, si hay o no fuentes de financiamiento disponibles, y un contacto para responder dudas sobre el proyecto.</p>
<p>Sé que es más difícil acceder a este tipo de anuncios. Quizás, en este caso, aconsejaría empezar la búsqueda por la universidad de interés antes que por el tópico y disciplina. En este sentido, al identificar una potencial universidad, quizás de acuerdo a algún ranking, revisar si para su área de interés existen proyectos de investigación asociados.</p>
<p>Por ejemplo, la <a href="https://www.uq.edu.au/">Universidad de Queensland</a>, en Brisbane, Australia, tiene una <a href="https://apply.uq.edu.au/">página de anuncios</a> de proyectos de investigación.</p>
<h4 id="utilizando-redes-sociales"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#utilizando-redes-sociales">Utilizando redes sociales</a></h4>
<p>Quizás suene extraño, pero muchos investigadores ofrecen sus proyectos de investigación a potenciales estudiantes a través de redes sociales como <a href="https://twitter.com/home">X</a> o <a href="https://www.linkedin.com/feed/">LinkedIn</a>. Una forma de acceder a estas oportunidades es escribir "PhD Position in Reinforcement Learning" en los buscadores de estas redes (por eso fundamental haber elegido al menos la disciplina), ajustando el filtro a las últimas publicaciones. De esta manera, siempre encontraba varias posiciones que podrían ser de mi interés cada cierto tiempo. A veces, es posible que no encuentres nada interesante, por lo que es necesario repetir la búsqueda periódicamente.</p>
<h3 id="cierre"><a class="toclink" href="2024/03/03/tips-para-buscar-phd-en-el-extranjero/#cierre">Cierre</a></h3>
<p>Finalmente, muchas gracias por leer! y espero, que estos consejos les sean de utilidad. También debo decir que buscar un programa de doctorado es una tarea difícil y frustrante que puede tomar meses quizás hasta años, pero como dije lo importante es no rendirse y creer en uno mismo, los sueños son para cumplirlos. Si les gustaría profundizar sobre el tema u otras dudas relacionadas, en la página de inicio se encuentran mis correos profesionales. De nuevo, gracias por leer y mucha suerte en la búsqueda! 😊😊😊</p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-02-29 00:00:00">February 29, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="category/hello/" class="md-meta__link">Hello</a>, 
              <a href="category/world/" class="md-meta__link">World</a></li>
        
        
          
          <li class="md-meta__item">
            
              2 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="mi-primer-blog-bagging-vs-boosting"><a class="toclink" href="2024/02/29/mi-primer-blog-bagging-vs-boosting/">Mi primer blog, Bagging vs Boosting.</a></h2>
<p>Hola,</p>
<p>Este es el primer blog que escribo. He decidido empezar a escribir una series de blogs relacionado con Machine Learning y Ciencia de Datos en español. Existe mucho material escrito en inglés, por lo que creo que no haré mucho aporte en ese espacio.</p>
<p>Recientemente comencé a buscar trabajo de manera formal y noté que tenía ciertas dificultades a la hora de responder las preguntas típicas en entrevistas para cargos relacionados a Machine Learning Engineer. Por esta razón, he decidido hacer un pequeño aporte educativo dirigido a personas en la misma situación en la que yo estuve, así como a aquellos que están comenzando en el fascinante mundo de la Inteligencia Artificial. </p>
<p>Un obstáculo común es la barrera idiomática, especialmente considerando que la mayoría de los mejores materiales y blogs están en inglés. Por ello, de todos los animo a perder el miedo a aprender un nuevo idioma, como el inglés.</p>
<p>Suficiente ya de introducción, este primer artículo es sobre la diferencias entre Bagging y Boosting, dos métodos de Ensamblaje de Modelos, que buscan mejorar el rendimiento de modelos predictivos en tareas de clasificación o regresión. La noción básica es combinar el aprendizaje adquirido de varios modelos en lugar de uno sólo, para mejorar el rendimiento en tareas predictivas.</p>
<p>Los dos métodos principales son Bagging y Boosting. El objetivos de estos métodos es reducir la varianza de un sólo estimador combinando varios estimadores de diferentes modelos, generando resultados con mayor estabilidad.</p>
<p>Ahora revisemos en profundidad como funcionan estas técnicas de ensamble:</p>
<h3 id="bagging"><a class="toclink" href="2024/02/29/mi-primer-blog-bagging-vs-boosting/#bagging">Bagging</a></h3>
<p>Boostrap Aggregation, también conocido como bagging, es una técnica de ensamble de Machine Learning, cuyo objetivo es mejorar la estabilidad y el rendimiento de los modelos de predictivos, principalmente en tareas de clasificación y regresión. Reduce la varianza y ayuda a evitar el sobre-entrenamiento en modelos de Machine Learning, un fenómeno que hace los modelos se aprendan de "memoria" los datos usados durante el entrenamiento, y a la hora de revisar información nunca antes vista el modelo tenga un pésimo rendimiento.</p>
<h3 id="boosting"><a class="toclink" href="2024/02/29/mi-primer-blog-bagging-vs-boosting/#boosting">Boosting</a></h3>
<h3 id="referencias"><a class="toclink" href="2024/02/29/mi-primer-blog-bagging-vs-boosting/#referencias">Referencias</a></h3>
    
  </div>
</article>
      
      
        
          



<nav class="md-pagination">
  
</nav>
        
      
    </div>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021 - 2025 Gabriel Iturra-Bocaz
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/giturra" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/giturra/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/g_iturrab" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.instagram.com/ilovethedoggos1/" target="_blank" rel="noopener" title="www.instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141m0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7m146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8m76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8M398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.footer"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>