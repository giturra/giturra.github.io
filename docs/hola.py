"Large Language Models (LLMs) are significant deep learning models trained on vast corpora of text. These models have revolutionized the field of Natural Language Processing (NLP) with their ability to accurately perform various NLP tasks, such as sentiment analysis, text classification, and text summarization. These advances have raised questions about whether these models could enhance the resolution in planning tasks using the reasoning knowledge that LLMs can provide. However, despite their groundbreaking advancements, it is not clear how to integrate the semantic knowledge into autonomous agents. This oversight is why I am deeply interested in approaches that improve planning and grounded tasks using the semantic knowledge that LLMs can provide. During my master’s degree, I developed RiverText, a framework for training and evaluating word embeddings from streaming text data. This framework employs multiple methods and algorithms to incrementally represent word embeddings based on the distributional hypothesis. My work received international recognition, being accepted at the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’23) in Taipei, Taiwan. I currently serve as a part-time professor in the Master of Data Science program at the University of Chile, where we explore the most critical aspects of Machine Learning and Data Mining. I am actively seeking Ph.D. opportunities to further explore the potential of Large Language Models, aiming to enhance their prompting engineering capabilities, broaden their applications in Robotics or Healthcare, and leverage their capabilities across various disciplines."